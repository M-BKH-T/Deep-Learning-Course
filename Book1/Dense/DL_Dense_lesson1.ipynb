{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff12f661-7c02-4c29-aa6a-e85cce28d8de",
   "metadata": {},
   "source": [
    "DL_DENSE_LESSON 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b3d3ac-b478-4cd4-b926-c2a9ee6bb930",
   "metadata": {},
   "source": [
    "in sequentioal lessons we learned how to creat a simple model <br>\n",
    "it this course i wanna show you how a dense layer works and what should we do to make it better"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31a8c3d-dc6d-4b8e-adc3-bb4a2b05800b",
   "metadata": {},
   "source": [
    "Dense class<br>\n",
    "keras.layers.Dense( <br>\n",
    "    units,<br>\n",
    "    activation=None,<br>\n",
    "    use_bias=True,<br>\n",
    "    kernel_initializer=\"glorot_uniform\",<br>\n",
    "    bias_initializer=\"zeros\",<br>\n",
    "    kernel_regularizer=None,<br>\n",
    "    bias_regularizer=None,<br>\n",
    "    activity_regularizer=None,<br>\n",
    "    kernel_constraint=None,<br>\n",
    "    bias_constraint=None,<br>\n",
    "    lora_rank=None,<br>\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318e003a-bed4-4cdc-b502-6aaf2e66fd43",
   "metadata": {},
   "source": [
    "as you see , Dense layer has many hyperparameters and  if you want to creat an awesom model , must know each ones doing what"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f779d80-187c-4bd4-bb52-1863dc0314bd",
   "metadata": {},
   "source": [
    "i wanna tell you all important arguments in dense layer <br>\n",
    "first lets talk about Af(activation function)<br>\n",
    "An Activation Function decides whether a neuron should be activated or not.<br> This means that it will decide whether the neuron's input to the network is important or not in the process of prediction using simpler mathematical operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce16cd4-5271-4503-bcd3-b7e293171758",
   "metadata": {},
   "source": [
    "In other words, the activation function determines the weight range of each neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52632ba-36f7-4ac1-ac26-a5ef0a8f8789",
   "metadata": {},
   "source": [
    "Sigmoid / Logistic Activation Function<br>\n",
    "This function takes any real value as input and outputs values in the range of 0 to 1. The larger the input (more positive), the closer the output value will be to 1.0, whereas the smaller the input (more negative), the closer the output will be to 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de01bd42-ed2f-43c1-9ce5-f7e7cd39b25e",
   "metadata": {},
   "source": [
    "Tanh (Hyperbolic Tangent) Activation<br>\n",
    "This means that it can deal with negative values more effectively than the sigmoid function, which has a range of 0 to 1. Unlike the sigmoid function, tanh is zero-centered, which means that its output is symmetric around the origin of the coordinate system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd76041-651c-4ec1-849f-de485f616634",
   "metadata": {},
   "source": [
    "The ReLU activation function is important in deep learning and machine learning because: It introduces non-linearity, allowing the network to learn complex patterns and relationships in the data. It helps in overcoming the vanishing gradient problem, which can hinder the learning process in deep neural networks.<br>\n",
    "As you can see, the derivative of all values ​​less than zero in this function is zero, and it causes some neurons to have only zero value (in other words, the neuron is killed).<br>\n",
    "what's the solution?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9f0d4c-8f8b-457d-86f9-01a0a673f789",
   "metadata": {},
   "source": [
    "The solution is to use a new activation function called LeakyReUL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b5a774-440a-4d62-b99f-928b68b7efb3",
   "metadata": {},
   "source": [
    "The Leaky ReLU is a popular activation function that is used to address the limitations of the standard ReLU function in deep neural networks by introducing a small negative slope for negative function inputs, which helps neural networks to maintain better information flow both during its training and after"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce567c2-e31b-40ad-ac28-80abca2c1dca",
   "metadata": {},
   "source": [
    "![](activation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea251ad-54d3-4761-a1a1-6b73725de18d",
   "metadata": {},
   "source": [
    "We have many other function for example:SELU , RRelu , PRelu , elu , Softplus and etc <br> [more imformation](https://en.wikipedia.org/wiki/Activation_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff557d5-81b4-4992-ac01-fc28da4b99f1",
   "metadata": {},
   "source": [
    "Maybe you ask , Which one should we use?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5844448b-c65e-41ff-9164-0149351d0ec1",
   "metadata": {},
   "source": [
    "If speed is more important to you, use Relu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6352ff25-f507-4abe-963f-09320265c4ff",
   "metadata": {},
   "source": [
    "But if you want improve youe model(**not in all models**) accuracy use LeakyRelu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73941d41-252d-4237-baef-6ce47ad0f141",
   "metadata": {},
   "source": [
    "Tanh is not used much anymore <br>\n",
    "so you shuld(better) choise between Relu family (elu , RRelu , PRelu , SeLu) and sigmoid(logistic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d013279-2bae-4549-9193-584920143b7e",
   "metadata": {},
   "source": [
    "Let's coding :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a10d9105-ed8d-47ac-899f-7544ddd86b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69907773-7d6a-44db-9b65-f0ebb2c46914",
   "metadata": {},
   "outputs": [],
   "source": [
    "fashin = keras.datasets.fashion_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be6016a-69d7-4af9-8916-9af166f7f350",
   "metadata": {},
   "outputs": [],
   "source": [
    "(xtrain , ytrain) , (xtest , ytest) = fashin.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2085a963-d837-4c94-81f7-bc3409362d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain , xtest = xtrain/255.0 , xtest/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b10d77a-4111-4f8b-82ca-df81bba073bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(100, activation=\"selu\"), # so easy for use relu elu selu sigmoid \n",
    "    keras.layers.Dense(75, activation=\"relu\"),# but for use RRelu do as below\n",
    "    keras.layers.Dense(50),\n",
    "    keras.layers.LeakyReLU(alpha=0.3), # for best performance set alpha between 0.2 and 0.3\n",
    "    keras.layers.Dense(10, activation=\"softmax\") # in last layer in classification always must use softmax\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793bbba3-8bdd-418b-981e-3bc18e029581",
   "metadata": {},
   "source": [
    "Tip : Remember **regression has no AF in last layer** "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
